
\section{Introduction}



\begin{frame}
 \frametitle{Outline}
  


\tableofcontents[currentsection]

  
 \end{frame}
\begin{frame}
 \frametitle{whoami}
  


\begin{itemize}
  \item Valentin Haenel
  \item Currently Masters Student in Berlin
  \item Working as student assistant in the lab of Felix Wichmann
  \item Did work on PyMVPA as part of a \textit{Lab Rotation}
  \item Worked in the lab of John Dylan Haynes
\end{itemize}

  
 \end{frame}
\begin{frame}
 \frametitle{What work?}
  


\begin{itemize}
  \item Compare the PyMVPA searchlight with Matlab
  \item Matlab script is lab internal
  \item A well defined algorithm should be easy to compare
  \item Compare for correctness and speed
  \item Use a dataset with known results
\end{itemize}

  
 \end{frame}
\begin{frame}
 \frametitle{Dataset}
  


\begin{block}{Dataset came from:}
S.Bode and J.D. Haynes. Decoding sequential stages of task preperation in the human brain.\\
\emph{Neuroimage}, 45(2):606-613, Apr 2009
\end{block}

\begin{itemize}
  \item Finite Impulse Response (FIR) Betas
  \item Linear C SVM, C = 1
  \item 4-Fold Crossvalidation
  \item Searchlight radius: 4 voxels/12 mm
\end{itemize}

  
 \end{frame}
\begin{frame}
 \frametitle{Initial Results}
  


One subject, all 8 FIR Timebins:

\begin{itemize}
  \item Accuracy maps $99.99\%$ equal
  \item Matlab + SPM  $\approx$ 22 minutes
  \item PyMVPA $\approx$ 2 Hours 43 minutes
\end{itemize}

\begin{itemize}
  \item Cluster Node
  \item 2 x Dual Core Athlon Opteron 2220
  \item 16 GB of Ram
  \item Debian 5.0 (Lenny), updated April 2009
\end{itemize}

  
 \end{frame}

\section{Timing and Improvements}



\begin{frame}
 \frametitle{Profiling}
  


\begin{itemize}
  \item Used standard profiling tools: cProfile and pstats
  \item \texttt{python -m cProfile -o runprof6 analysis.py}
  \item Sort stats using option: \texttt{time}
  \item Looking at callees and callers can be useful
  \item Try to get a feel first
  \item Output will not fit on the slides, show in a shell instead
\end{itemize}

\begin{itemize}
  \item Post mortem reconstruction of the process
  \item Forgot to annotate the session with commit sha-1 :-(
\end{itemize}

  
 \end{frame}
\begin{frame}
 \frametitle{runprof1 - Comments}
  


\begin{itemize}
  \item Probably done on a single FIR timebin
  \item \texttt{\_svm.py:179(convert2SVMNode)} is obvious
  \item Thus: look at that function
  \item We see lots of unnecessary appends
  \item So lets fix something easy first
\end{itemize}

  
 \end{frame}
\begin{frame}
 \frametitle{Commit 1 - 8d9237d}
  


\begin{itemize}
  \item Optimizations at the python level
  \item Factor out the appends
  \item introduce
\begin{itemize}
  \item constructor
  \item \texttt{range} function
  \item list comprehension
\end{itemize}
\end{itemize}

  
 \end{frame}
\begin{frame}
 \frametitle{runprof2 - Comments}
  


\begin{itemize}
  \item Probably done on a single FIR timebin
  \item The \texttt{append} method has disappeared
  \item This is great but the original bottleneck is still present
  \item So lets try something more radical...
\end{itemize}

  
 \end{frame}
\begin{frame}
 \frametitle{Commit 2 - 6fd8bae}
  


\begin{itemize}
  \item Tricky optimization involving SWIG wrapper
  \item Move loops from python to C
  \item Make use of the Python C API
\end{itemize}

  
 \end{frame}
\begin{frame}
 \frametitle{runprof3 - Comments}
  


\begin{itemize}
  \item Probably done on a single FIR timebin
  \item All of the SVM stuff is gone
  \item However we see the following
\begin{itemize}
  \item mask.py:216(isValidInId)
  \item mask.py:221(getOutId)
  \item support.py:306(isInVolume)
\end{itemize}
  \item Some intelligent guesswork leads us to believe that this might be caused by the searchlight algorithm
\end{itemize}

  
 \end{frame}
\begin{frame}
 \frametitle{Commit 3 - origin/val/new-sl}
  


\begin{itemize}
  \item Unfortunately still unmerged - maybe this can be done during the workshop
  \item Basic idea is to cache the searchlights when operating on FIR datasets
  \item The assumption is that each dataset in a list of datasets came from the same brain
  \item Hence no need to recompute the searchlights every time
  \item Diff is slightly more involved so will not be presented here
\end{itemize}

  
 \end{frame}
\begin{frame}
 \frametitle{runprof4 - comments}
  


\begin{itemize}
  \item Probably done on all 8 timebins
  \item We now see that our guesswork was fruitfull
  \item Most references to \texttt{mask.py} and \texttt{support.py} have dissapeared
  \item We see the infamous \texttt{state.py:306(\_\_getattribute\_\_)} turn up.
  \item This was the last real improvement, runprof5 and runprof6 don't make real gain
\end{itemize}

  
 \end{frame}
\begin{frame}
 \frametitle{Commit 4 - 7de1069}
  


\begin{itemize}
  \item Instead of converting our ndarray to a list, we just pass the ndarray directly
  \item Make use of Numpy C API
\end{itemize}

  
 \end{frame}
\begin{frame}
 \frametitle{runprof5 - comments}
  


\begin{itemize}
  \item Probably done on all 8 timebins
  \item \texttt{\{method 'tolist' of 'numpy.ndarray' objects\}} Has disappeared.
\end{itemize}

  
 \end{frame}
\begin{frame}
 \frametitle{Commit 5 - d8490ad}
  


\begin{itemize}
  \item Remove a defensive deepcopy only needed when sorting
  \item Just noticed: searchlight should be adapted to use this (!)
  \item Just noticed: could be improved further
\end{itemize}

  
 \end{frame}
\begin{frame}
 \frametitle{runprof6 - comments}
  


\begin{itemize}
  \item Probably done on all 8 timebins
  \item We see that \texttt{copy.py:144(deepcopy)} has disappeared.
  \item \texttt{state.py} now holds the top 5 slots in the profiler output
  \item I guess optimizing this would be beneficial to all of PyMVPA
  \item The question is just how to do it?
  \item Suggestion: use property injection to do the delegation
\end{itemize}

  
 \end{frame}
\begin{frame}
 \frametitle{Some last words on timing}
  


\begin{tabular}{ l l  }
description                    &  time                              \\
No improvements                &  2 hrs 43 mins and 27 secs \\
No improvements WITH -O        &  2 hrs 5  mins and 44 secs \\
New Searchlight                &  2 hrs 10 mins and 28 secs \\
New Searchlight WITH -O        &  1 hrs 28 mins and 26 secs \\
New LIBSVM Wrapper             &  1 hrs 40 mins and 24 secs \\
New LIBSVM Wrapper WITH -O     &  1 hrs 34 mins and 26 secs \\
Searchlight + LIBSVM           &  0 hrs 58 mins and 27 secs \\
Searchlight + LIBSVM WITH -O   &  0 hrs 52 mins and 18 secs \\
\end{tabular}

  
 \end{frame}
\begin{frame}
 \frametitle{LIBSVM Wrapper}
  


\begin{itemize}
  \item I understand that its from upstream?
  \item Does it make sense to backpropagate/merge/fork/separate?
  \item MDP uses SVMs too?
  \item Need for a PyLIBSVM project? (!)
\end{itemize}

  
 \end{frame}

\section{Conclusions}



\begin{frame}
 \frametitle{The Pains and Perils of Optimization}
  


\begin{block}{Common saying amongst Pythonistas:}
First get it right... then make it fast!
\end{block}

\hspace{1cm}

\begin{block}{Quoting Donald Knuth, author of}
Premature Optimization is the root of all evil.
\end{block}

\hspace{1cm}

\begin{block}{Quote from the PyMVPA Mailinglist}
[...]in our craving for being generic we sacrificed \\
quite a bit in terms of the performance.[...]
\end{block}

  
 \end{frame}
\begin{frame}
 \frametitle{Conclusion}
  


\begin{itemize}
  \item Look for spots of maximum reward with minimum gain
  \item Its always an iterative process, patience is a virtue
  \item Optimize only what you need
  \item Use the right tools
  \item Measure your success
\end{itemize}


  
 \end{frame}

